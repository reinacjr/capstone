{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b999c3-1beb-45b8-8d71-0cef650118e7",
   "metadata": {},
   "source": [
    "# Hard-coded Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b0bcf65-0a5e-4568-b353-cafb15e56dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#Step 1: Loading the 3 Data Files: Data Extraction Fields (Excel sheet that can be amended anytime), Planned Test Cases Data, Log Files\n",
    "\n",
    "#Loading the list of desired output fields from the Excel, file path should be updated accordingly to user's own file path\n",
    "fields_df = pd.read_excel(\"C:/Users/zolyn/OneDrive/Desktop/BT4103/Files to Parse In/Data Extraction Fields.xlsx\", header=None)\n",
    "desired_fields = fields_df[0].dropna().tolist()\n",
    "\n",
    "# Load the planned test cases CSV file, file path should be updated accordingly to user's own file path\n",
    "planned_df = pd.read_csv(\"C:/Users/zolyn/OneDrive/Desktop/BT4103/Files to Parse In/refined_security_test_cases.csv\")\n",
    "\n",
    "# Read and split logs by separator, file path should be updated accordingly to user's own file path\n",
    "with open(\"C:/Users/zolyn/OneDrive/Desktop/BT4103/Files to Parse In/final_updated_logs.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    log_lines = f.read().split(\"======================================================\")\n",
    "\n",
    "#Step 2: Parsing Logs in Pairs (Request + Response)\n",
    "\n",
    "paired_log_entries = []\n",
    "i = 0\n",
    "while i < len(log_lines) - 1:\n",
    "    req_block = log_lines[i].strip()\n",
    "    res_block = log_lines[i + 1].strip()\n",
    "    i += 2  #Move to the next pair\n",
    "\n",
    "    lines = [line.strip() for line in req_block.splitlines() if line.strip()]\n",
    "    log_info = {\n",
    "        \"Date of Test Executed\": \"\",\n",
    "        \"Project ID\": \"\",\n",
    "        \"User ID\": \"\",\n",
    "        \"Test Case ID\": \"\",\n",
    "        \"User Agent\": \"\",\n",
    "        \"Request Method\": \"\",\n",
    "        \"Requested Source\": \"\",\n",
    "        \"Test Case Outcome Message\": \"\",\n",
    "        \"Error Message\": \"\",\n",
    "        \"Response Size\": \"\"\n",
    "    }\n",
    "\n",
    "    for line in lines:\n",
    "        if re.match(r\"^(GET|POST|PUT|DELETE|HEAD|OPTIONS|PATCH)\", line):\n",
    "            log_info[\"Request Method\"] = line.split()[0]\n",
    "            log_info[\"Requested Source\"] = line.split()[1]\n",
    "        elif \"User-Agent:\" in line:\n",
    "            log_info[\"User Agent\"] = line.split(\"User-Agent:\")[-1].strip()\n",
    "        elif \"Project ID:\" in line:\n",
    "            log_info[\"Project ID\"] = line.split(\"Project ID:\")[-1].strip()\n",
    "        elif \"User ID:\" in line:\n",
    "            log_info[\"User ID\"] = line.split(\"User ID:\")[-1].strip()\n",
    "        elif \"Test Case ID:\" in line:\n",
    "            log_info[\"Test Case ID\"] = line.split(\"Test Case ID:\")[-1].strip()\n",
    "\n",
    "    #Parse the corresponding response block\n",
    "    lines = [line.strip() for line in res_block.splitlines() if line.strip()]\n",
    "    for line in lines:\n",
    "        if line.startswith(\"HTTP\"):\n",
    "            log_info[\"Test Case Outcome Message\"] = \"Pass\" if \"200 OK\" in line else \"Fail\"\n",
    "            log_info[\"Error Message\"] = \"\" if \"200 OK\" in line else line\n",
    "        elif line.lower().startswith(\"date:\"):\n",
    "            log_info[\"Date of Test Executed\"] = line.split(\":\", 1)[-1].strip()\n",
    "        elif \"Content-Length:\" in line:\n",
    "            log_info[\"Response Size\"] = line.split(\"Content-Length:\")[-1].strip()\n",
    "\n",
    "    if log_info[\"Test Case ID\"] and log_info[\"Project ID\"] and log_info[\"User ID\"]:\n",
    "        paired_log_entries.append(log_info)\n",
    "\n",
    "#Step 3: Merge Logs with Planned Test Cases\n",
    "\n",
    "logs_df = pd.DataFrame(paired_log_entries)\n",
    "merged_df = pd.merge(\n",
    "    planned_df, logs_df,\n",
    "    on=[\"Project ID\", \"User ID\", \"Test Case ID\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#Add manual input columns\n",
    "merged_df[\"Actual Test Case Outcome\"] = \"\"\n",
    "merged_df[\"Reason for failure/success\"] = \"\"\n",
    "\n",
    "#Step 4: Match Column Names Case-Insensitive\n",
    "\n",
    "#Normalize column names to match even with case differences\n",
    "column_mapping = {col.lower(): col for col in merged_df.columns}\n",
    "normalized_desired = [col.lower() for col in desired_fields]\n",
    "final_columns = [column_mapping[col] for col in normalized_desired if col in column_mapping]\n",
    "\n",
    "#Filter only final export columns\n",
    "final_output = merged_df[final_columns]\n",
    "\n",
    "#Step 5: Export to CSV, file path should be updated accordingly to user's own file path\n",
    "final_output.to_csv(\"C:/Users/zolyn/OneDrive/Desktop/BT4103/Output Files from Logs Standardisation System/hardcode_logs_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e65a9d-6364-435c-9df3-c78ea52ee782",
   "metadata": {},
   "source": [
    "# More Dynamic Version of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85437fb4-0209-49b0-a4c7-0dc1ddfb418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#STEP 1: Load Files\n",
    "\n",
    "#Excel: list of export fields, file path should be updated accordingly to user's own file path\n",
    "field_df = pd.read_excel(\"C:/Users/zolyn/OneDrive/Desktop/BT4103/Files to Parse In/Data Extraction Fields.xlsx\", header=None)\n",
    "field_df.columns = [\"Field Name\", \"Manual Input\"]\n",
    "field_df[\"Field Name\"] = field_df[\"Field Name\"].str.strip()\n",
    "field_df[\"Manual Input\"] = field_df[\"Manual Input\"].fillna(\"\").astype(str).str.lower()\n",
    "\n",
    "#Extract desired fields and which ones are manual input fields\n",
    "desired_fields = field_df[\"Field Name\"].tolist()\n",
    "manual_fields = field_df[field_df[\"Manual Input\"].isin([\"yes\", \"true\", \"manual\"])][\"Field Name\"].tolist()\n",
    "\n",
    "#CSV: planned test cases, file path should be updated accordingly to user's own file path\n",
    "planned_df = pd.read_csv(\"C:/Users/zolyn/OneDrive/Desktop/BT4103/Files to Parse In/refined_security_test_cases.csv\")\n",
    "\n",
    "#TXT: raw logs, file path should be updated accordingly to user's own file path\n",
    "with open(\"C:/Users/zolyn/OneDrive/Desktop/BT4103/Files to Parse In/final_updated_logs.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    log_blocks = f.read().split(\"======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da8c82c4-166f-4acb-abce-c21343510084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: Parse Logs in Pairs\n",
    "\n",
    "parsed_logs = []\n",
    "i = 0\n",
    "while i < len(log_blocks) - 1:\n",
    "    req = log_blocks[i].strip()\n",
    "    res = log_blocks[i + 1].strip()\n",
    "    i += 2\n",
    "\n",
    "    entry = {\n",
    "        \"Date of Test Executed\": \"\",\n",
    "        \"Project ID\": \"\",\n",
    "        \"User ID\": \"\",\n",
    "        \"Test Case ID\": \"\",\n",
    "        \"User Agent\": \"\",\n",
    "        \"Request Method\": \"\",\n",
    "        \"Requested Source\": \"\",\n",
    "        \"Test Case Outcome Message\": \"\",\n",
    "        \"Error Message\": \"\",\n",
    "        \"Response Size\": \"\"\n",
    "    }\n",
    "\n",
    "    for line in req.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if re.match(r\"^(GET|POST|PUT|DELETE|HEAD|OPTIONS|PATCH)\", line):\n",
    "            parts = line.split()\n",
    "            entry[\"Request Method\"] = parts[0]\n",
    "            entry[\"Requested Source\"] = parts[1]\n",
    "        elif \"User-Agent:\" in line:\n",
    "            entry[\"User Agent\"] = line.split(\"User-Agent:\")[-1].strip()\n",
    "        elif \"Project ID:\" in line:\n",
    "            entry[\"Project ID\"] = line.split(\"Project ID:\")[-1].strip()\n",
    "        elif \"User ID:\" in line:\n",
    "            entry[\"User ID\"] = line.split(\"User ID:\")[-1].strip()\n",
    "        elif \"Test Case ID:\" in line:\n",
    "            entry[\"Test Case ID\"] = line.split(\"Test Case ID:\")[-1].strip()\n",
    "\n",
    "    for line in res.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith(\"HTTP\"):\n",
    "            entry[\"Test Case Outcome Message\"] = \"Pass\" if \"200 OK\" in line else \"Fail\"\n",
    "            entry[\"Error Message\"] = \"\" if \"200 OK\" in line else line\n",
    "        elif line.lower().startswith(\"date:\"):\n",
    "            entry[\"Date of Test Executed\"] = line.split(\":\", 1)[-1].strip()\n",
    "        elif \"Content-Length:\" in line:\n",
    "            entry[\"Response Size\"] = line.split(\"Content-Length:\")[-1].strip()\n",
    "\n",
    "    if entry[\"Project ID\"] and entry[\"User ID\"] and entry[\"Test Case ID\"]:\n",
    "        parsed_logs.append(entry)\n",
    "\n",
    "logs_df = pd.DataFrame(parsed_logs)\n",
    "\n",
    "#STEP 3: Merge Logs with Planned Cases\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    planned_df,\n",
    "    logs_df,\n",
    "    on=[\"Project ID\", \"User ID\", \"Test Case ID\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#STEP 4: Add Manual Input Fields if Missing\n",
    "\n",
    "for col in manual_fields:\n",
    "    normalized_col = col.strip().lower()\n",
    "    if normalized_col not in [c.strip().lower() for c in merged_df.columns]:\n",
    "        merged_df[col] = \"\"\n",
    "\n",
    "#STEP 5: Normalise and Align Columns\n",
    "\n",
    "# Normalize for case-insensitive column matching\n",
    "merged_df.columns = [col.strip().lower() for col in merged_df.columns]\n",
    "normalized_columns_map = {col.lower(): col for col in desired_fields}\n",
    "final_cols_lower = [col.strip().lower() for col in desired_fields]\n",
    "\n",
    "#Ensure all desired fields exist\n",
    "for col in final_cols_lower:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = \"\"\n",
    "\n",
    "#Reorder and restore original column names\n",
    "final_output = merged_df[final_cols_lower]\n",
    "final_output.columns = desired_fields  # Pretty headers\n",
    "\n",
    "#STEP 6: Export to CSV, file path should be updated accordingly to user's own file path\n",
    "final_output.to_csv(\"C:/Users/zolyn/OneDrive/Desktop/BT4103/Output Files from Logs Standardisation System/dynamic_logs_output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
